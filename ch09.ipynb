{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b59a3da-3362-437e-ab43-8fc8ddcee57e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Networks from Scratch\n",
    "### Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c118e26",
   "metadata": {},
   "source": [
    "Below is an example of a neural network with 3 input neurons and the full forward pass through a single neuron and a ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c58454c-bfb9-43ea-a108-b4f0e186434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29643eb6-c4aa-4998-af2a-f57c0935adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0 1.0\n"
     ]
    }
   ],
   "source": [
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "print(xw0, xw1, xw2, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe44b55-9c79-4ab7-a9a8-7b7e5cc19da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "z = xw0 + xw1 + xw2 + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b27d792-f3e2-4eb2-adad-278a46005d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e9cc8",
   "metadata": {},
   "source": [
    "The full forward pass can be treat as a big function, as specified below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bea073",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLU(\\sum_{i=0}^{3} x_{i}w_{i} + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36f360",
   "metadata": {},
   "source": [
    "We want to know, how to adjust $w$ such that the cost function $C$ is minimized. For that, we can use a gradient descent algorithm which requires the derivative of the function above with respect to $w_{i}$. The derivative of the function above is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d54862",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial w}[ReLU(\\sum_{i=0}^{3} x_{i}w_{i} + b)] = $\\frac{\\partial ReLU(\\sum_{i=0}^{3} x_{i}w_{i} + b)}{\\partial \\sum_{i=0}^{3} x_{i}w_{i} + b} * \\frac{\\partial \\sum_{i=0}^{3} x_{i}w_{i} + b}{\\partial x_{i}w_{i}} * \\frac{\\partial x_{i}w_{i}}{\\partial w_{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c9d77",
   "metadata": {},
   "source": [
    "Let's assume that our neuron receives a gradient of 1 from the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1077245-1c27-4a22-b565-1a547af35492",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalue = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5125170",
   "metadata": {},
   "source": [
    "The derivarive of ReLU is (1. if z > 0 else 0.). Then we multiply it by the value received from the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "978b62bb-4fa3-4d2c-aab1-4a9f9fea86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drelu_dz = dvalue * (1. if z > 0 else 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0829f26a-c870-43d6-af8b-8cbe62299f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(drelu_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b3dec",
   "metadata": {},
   "source": [
    "Next, we compute the gradient for each $w_{i}$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb4444b-aaa0-4f85-9fe7-dd3a807eb217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf500be4",
   "metadata": {},
   "source": [
    "Finally is the gradients for the inputs and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a5caa54-1026-4e7f-bb52-8dff5bd09f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e55ada",
   "metadata": {},
   "source": [
    "Here are our gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2afee120-d4b3-45af-a78a-77c7317df6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bad271",
   "metadata": {},
   "source": [
    "Applying the negative of the gradients to our weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56816367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 -1.0 2.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(*w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1506e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e64e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.001 -0.998 1.997 0.999\n"
     ]
    }
   ],
   "source": [
    "print(*w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b90d5",
   "metadata": {},
   "source": [
    "And here is another forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46fbee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.001 1.996 5.9910000000000005 0.999\n",
      "5.985\n",
      "5.985\n"
     ]
    }
   ],
   "source": [
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "print(xw0, xw1, xw2, b)\n",
    "\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(z)\n",
    "\n",
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c5f02",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94bd6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([1.,1.,1.])\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20266059",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx0 = sum(weights[0])*dvalues[0]\n",
    "dx1 = sum(weights[1])*dvalues[0]\n",
    "dx2 = sum(weights[2])*dvalues[0]\n",
    "dx3 = sum(weights[3])*dvalues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f3f5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinputs = np.array([dx0, dx1, dx2, dx3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a83cdda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f33e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b450562d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44, -0.38, -0.07,  1.37])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalues = np.array([[1.,1.,1.]])\n",
    "d_inputs = np.dot(dvalues[0], weights.T)\n",
    "d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a06299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a35d2ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44, -0.38, -0.07,  1.37],\n",
       "       [ 0.88, -0.76, -0.14,  2.74],\n",
       "       [ 1.32, -1.14, -0.21,  4.11]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalues = np.array([[1.,1.,1.], [2.,2.,2.], [3.,3.,3.]])\n",
    "d_inputs = np.dot(dvalues, weights.T)\n",
    "d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5082b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ee86d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.]])\n",
    "\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "[2., 5., -1., 2],\n",
    "[-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "549e184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.]])\n",
    "\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0c24885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "[2, -7, -1, 3],\n",
    "[-1, 2, 5, -1]])\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "[5, 6, 7, 8],\n",
    "[9, 10, 11, 12]])\n",
    "# ReLU activation's derivative\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z > 0] = 1\n",
    "print(drelu)\n",
    "# The chain rule\n",
    "drelu *= dvalues\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7ec0308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4.8  ,  1.21 ,  2.385],\n",
       "        [ 8.9  , -1.81 ,  0.2  ],\n",
       "        [ 1.41 ,  1.051,  0.026]]),\n",
       " array([[4.8  , 1.21 , 2.385],\n",
       "        [8.9  , 0.   , 0.2  ],\n",
       "        [1.41 , 1.051, 0.026]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2., 5., -1., 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum(0, layer_outputs)\n",
    "layer_outputs, relu_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a350a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.8  , 1.21 , 2.385],\n",
       "       [8.9  , 0.   , 0.2  ],\n",
       "       [1.41 , 1.051, 0.026]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "drelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "389aa66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.561562  , -1.8269043 ,  1.83359695],\n",
       "       [ 6.522304  , -4.8328256 , -0.3403356 ],\n",
       "       [-0.7261887 , -1.98542257, -0.50381082]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc9da63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2., 5., -1., 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum(0, layer_outputs) # ReLU activation\n",
    "\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd2afc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(np.array(inputs), self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "   \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "            single_dvalues)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class LossCategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
